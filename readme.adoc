:img-size: 200
:toc: macro
++++
<div align="center">
++++
= Colletions of Image Generation Models

image:https://img.shields.io/badge/-Python 3.7--3.9-blue?style=for-the-badge&logo=python&logoColor=white[python, link=https://pytorch.org/get-started/locally/]
image:https://img.shields.io/badge/-PyTorch 1.8+-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white[pytorch, link=https://pytorch.org/]
image:https://img.shields.io/badge/-Lightning 1.3+-792ee5?style=for-the-badge&logo=pytorchlightning&logoColor=white[pytorch_lignthing, link=https://www.pytorchlightning.ai/]
image:https://img.shields.io/badge/config-hydra 1.1-89b8cd?style=for-the-badge&labelColor=gray[hydra, link=https://hydra.cc/]

An easily scalable and hierachical framework including lots of image generation method with various datasets.

++++
</div>
<br> <br>
++++

Highlights ðŸ’¡:
[Highlights:]
* Various types of image generation methods(__Continuous updating__): 
** GANs: WGAN, InfoGAN, BiGAN
** VAEs: VQ-VAE, Beta-VAE, FactorVAE
** Augoregressive Models: PixelCNN
** Diffusion Models: DDPM
* Decomposition of model training, datasets and networks:
+
[source, bash]
----
python run.py model=wgan networks=conv64 datamodule=celeba exp_name=wgan/celeba_conv64
----
* Hierachical configuration of experiment in yaml file
** Manual change of configs in `configs/model`, `configs/datamodule` and `configs/networks`
** Run predefined experiments in `configs/experiment`
+
[source, bash]
----
python run.py experiment=vanilla_gan/cifar10
----
** Override hyperparameters from command line
+
[source, bash]
----
python run.py experiment=vanilla_gan/cifar10 model.lrG=1e-3 model.lrD=1e-3 exp_name=vanilla_gan/custom_lr
----
* Run multiple experiments at the same time:
** Grid search of hyperparameters:
+
[source, bash]
----
python run.py experiment=vae/mnist_conv model.lr=1e-3,5e-4,1e-4 "exp_name=vae/lr_${model.lr}"
----
** Run multiple experiments from config files:
+
[source, bash]
----
python run.py -m experiment=vae/mnist_conv,vae/cifar10,vae/celeba
----


toc::[]


== Setup

== Project Structure

== Generative Adversarial Networks(GANs)

=== GAN
*_Generative adversarial nets._* +
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio* +
NeurIPS 2014. [https://arxiv.org/abs/1406.2661[PDF]] [https://arxiv.org/abs/1701.00160[Tutorial]]

[cols="4*", options="header"] 
|===
^| Dataset
^| MNIST
^| CelebA
^| CIFAR10

^.^| Results
| image:assets/gan/mnist.jpg[mnist_mlp, {img-size}, {img-size}]
| image:assets/gan/celeba.jpg[cleba_conv, {img-size}, {img-size}]
| image:assets/gan/cifar10.jpg[cifar10_conv, {img-size}, {img-size}]
|===

=== WGAN


=== WGAN-GP

=== VAE-GAN

=== BiGAN/AIL


== Variational Autoencoders(VAEs)

=== VAE
*_Auto-Encoding Variational Bayes._* +
Diederik P.Kingma, Max Welling. +
ICLR 2014. [https://arxiv.org/abs/1312.6114[PDF]]

[cols="4*", options="header"] 
|===
^| Dataset
^| MNIST
^| CelebA
^| CIFAR10

^.^| Results
| image:assets/vae/mnist.jpg[mnist_mlp, {img-size}, {img-size}]
| image:assets/vae/celeba.jpg[cleba_conv, {img-size}, {img-size}]
| image:assets/vae/cifar10.jpg[cifar10_conv, {img-size}, {img-size}]
|===

=== BetaVAE

=== FactorVAE

=== AAE

=== AGE

== Diffusion Models
=== DDPM
